<html>
    <head>
      <link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css" />
      <script defer src="https://pyscript.net/latest/pyscript.js"></script>
      <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
    </head>

  <body>
    <b><p>Today is <u><label id='today'></label></u></p></b>
    <br>
    <div id="pi" class="alert alert-primary"></div>
    <py-env>
        - requests
        - beautifulsoup4
        - ScraperAPIClient
    </py-env>
    <py-script>
        import requests
        from requests import *
        from bs4 import BeautifulSoup
        import time
        from urllib.request import urlopen
        import json
        from scraper_api import ScraperAPIClient
        # store the URL in url as 
        # parameter for urlopen
        isbn = "9780321543257"
        # isbn = "1118290275"

        url = "https://openlibrary.org/isbn/"+isbn+".json"
        
        # store the response of URL
        response = urlopen(url)
        
        # storing the JSON response 
        # from url in data
        data_json = json.loads(response.read())

        # formats the data_json to be readable
        data_json_str = json.dumps(data_json, indent=2)
        
        # print the json response
        # print(data_json_str)

        title = data_json["title"]

        title = title.replace("(", "")
        title = title.replace(")", "")
        title = title.replace("TM", "")
        title = title.replace(" ", "-")
        title = title.lower()



        valoreBooks = "https://www.valorebooks.com/textbooks/"+title+"/"+isbn

        client = ScraperAPIClient('95a43ead4493ef31bbf4704a4bfaf81a')
        result = client.get(url = valoreBooks).text
        print(result)
        # Scrapy users can simply replace the urls in their start_urls and parse function
        # Note for Scrapy, you should not use DOWNLOAD_DELAY and
        # RANDOMIZE_DOWNLOAD_DELAY, these will lower your concurrency and are not
        # needed with our API
        # ...other scrapy setup code
        start_urls =[client.scrapyGet(url = 'http://httpbin.org/ip')]
        def parse(self, response):
        # ...your parsing logic here
            yield scrapy.Request(client.scrapyGet(url = 'http://httpbin.org/ip'), self.parse)



        soup = BeautifulSoup(result, 'lxml')

        first = soup.find_all('span', class_ = 'commaFormat')

        print(first)

        print(type(first))

        x = 1
        used = ""
        new = ""
        alternate = ""
        for points in first:
            if x == 1:
                new = str(points.text)
            elif x == 2:
                used = str(points.text)
            elif x == 3:
                alternate = str(points.text)
            x = x + 1

    </py-script>
  </body>
</html>